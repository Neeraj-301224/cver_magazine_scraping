# Scraped Data Processing Guide

## Overview

All scraped JSON files are now saved to the `scraped_data/` folder, and `insert_event.py` automatically processes all JSON files from this folder while checking for duplicates.

## Folder Structure

```
event_scraping/
â”œâ”€â”€ scraped_data/          â† All JSON files saved here
â”‚   â”œâ”€â”€ bhf.json
â”‚   â”œâ”€â”€ mindfulnessassociation.json
â”‚   â”œâ”€â”€ letsdothis.json
â”‚   â””â”€â”€ ... (all spider JSON files)
â”œâ”€â”€ insert_event.py       â† Processes all JSON files
â””â”€â”€ ...
```

## How It Works

### 1. Running Spiders

When you run any spider (e.g., `python run_bhf_spider.py`), the JSON output is automatically saved to:
- **Location:** `event_scraping/scraped_data/{spider_name}.json`
- **Example:** `event_scraping/scraped_data/bhf.json`

### 2. Processing JSON Files

Run `insert_event.py` to process all JSON files:

```bash
cd event_scraping
python insert_event.py
```

This will:
1. âœ… Find all `*.json` files in `scraped_data/` folder
2. âœ… Check each event against the database for duplicates
3. âœ… Only insert new events (skips duplicates)
4. âœ… Show detailed progress and summary

### 3. Duplicate Detection

Events are checked for duplicates using:
- **Primary:** URL matching (most reliable)
- **Fallback:** Name + Date combination

If an event already exists, it's skipped and not inserted again.

## Features

### Automatic Folder Creation
- The `scraped_data/` folder is created automatically if it doesn't exist
- All spiders save their JSON files there

### Duplicate Prevention
- Before inserting, checks if event exists in database
- Skips geocoding for events that already exist (if enabled)
- Prevents duplicate database entries

### Batch Processing
- Processes all JSON files in one run
- Shows progress for each file
- Provides summary statistics

## Usage Examples

### Process All JSON Files
```bash
cd event_scraping
python insert_event.py
```

### Process Specific Folder
```python
from insert_event import main
main(json_folder="path/to/custom/folder")
```

## Output Example

```
Found 5 JSON file(s) in 'scraped_data'
================================================================================

ğŸ“„ Processing file: bhf.json
--------------------------------------------------------------------------------
  Found 25 event(s) in bhf.json
  [1/25] â• Inserting: London Marathon 2025
      âœ… Successfully inserted (post ID: 1234)
  [2/25] â­ï¸  Skipping duplicate: London 10K (exists as post ID: 567)
  ...

ğŸ“Š File Summary for bhf.json:
     âœ… Successful: 20
     â­ï¸  Duplicates: 5
     âŒ Failed: 0

================================================================================
ğŸ“Š FINAL SUMMARY
================================================================================
Total JSON files processed: 5
Total events found: 125
âœ… Successfully inserted: 95
â­ï¸  Duplicates skipped: 30
âŒ Failed: 0
================================================================================
```

## Database Check Before Geocoding

To enable database checking before geocoding (saves API calls), set in your spider:

```python
def __init__(self, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.check_db_before_geocoding = True  # Enable DB check
```

Then when calling geocode_address, pass event data:

```python
# In parse_event method
item = EventScrapingItem()
# ... populate item ...

# Check and geocode only if event is new
coords = self.geocode_address(address, event_data=dict(item))
```

## Benefits

âœ… **Organized:** All JSON files in one folder  
âœ… **Efficient:** Batch processing of all files  
âœ… **Smart:** Automatic duplicate detection  
âœ… **Fast:** Skips geocoding for existing events  
âœ… **Safe:** Won't create duplicate database entries  

